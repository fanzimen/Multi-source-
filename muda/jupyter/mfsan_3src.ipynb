{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOsjoL/kPbI9GNqL2MNHLE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kp3IDiMrsyYT","executionInfo":{"status":"ok","timestamp":1673182420286,"user_tz":-480,"elapsed":23446,"user":{"displayName":"zm f","userId":"13806944385549967444"}},"outputId":"1b96a19c-dd6a-40f6-f792-a9e8b0557224"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/MyDrive')\n"]},{"cell_type":"code","source":["import argparse\n","import time\n","import os\n","import numpy as np\n","import random\n","import torch\n","import RUL.MutiSource.model_mfsan as model_mfsan\n","import RUL.MutiSource.data_loader as data_loader\n","from RUL.MutiSource.utils import get_free_gpu, weight_init, weight_init2, log_in_file, visualize_total_loss, save_model,score_cal,rmse_cal,load_model\n","import math\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import RUL.MutiSource.utils as utils\n","import pandas as pd\n","pd.set_option('mode.chained_assignment', None)"],"metadata":{"id":"mbdq0X0AtO-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchvision.datasets as datasets\n","from torch.utils.data import SubsetRandomSampler, DataLoader\n","from torchvision import transforms\n","import torch\n","import logging\n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","import torch\n","from torch.utils.data import Dataset\n","\n","def gen_sequence(id_df, seq_length, seq_cols):\n","    data_matrix = id_df[seq_cols].values\n","    num_elements = data_matrix.shape[0] #矩阵行数\n","\n","    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n","        yield data_matrix[start:stop, :]\n","\n","\n","def gen_labels(id_df, seq_length, label):\n","    data_matrix = id_df[label].values\n","    num_elements = data_matrix.shape[0]\n","\n","    return data_matrix[seq_length:num_elements, :]\n","\n"],"metadata":{"id":"kxFWQjh29Urm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#path\n","current_dir = \"/content/drive/MyDrive/RUL\"\n","train_FD001_path = current_dir +'/cmapss/train_FD001.csv'\n","test_FD001_path = current_dir +'/cmapss/test_FD001.csv'\n","RUL_FD001_path = current_dir+'/cmapss/RUL_FD001.txt'\n","FD001_path = [train_FD001_path, test_FD001_path, RUL_FD001_path]\n","\n","train_FD002_path = current_dir +'/cmapss/train_FD002.csv'\n","test_FD002_path = current_dir +'/cmapss/test_FD002.csv'\n","RUL_FD002_path = current_dir +'/cmapss/RUL_FD002.txt'\n","FD002_path = [train_FD002_path, test_FD002_path, RUL_FD002_path]\n","\n","train_FD003_path = current_dir +'/cmapss/train_FD003.csv'\n","test_FD003_path = current_dir +'/cmapss/test_FD003.csv'\n","RUL_FD003_path = current_dir +'/cmapss/RUL_FD003.txt'\n","FD003_path = [train_FD003_path, test_FD003_path, RUL_FD003_path]\n","\n","train_FD004_path =current_dir +'/cmapss/train_FD004.csv'\n","test_FD004_path = current_dir +'/cmapss/test_FD004.csv'\n","RUL_FD004_path = current_dir +'/cmapss/RUL_FD004.txt'\n","FD004_path = [train_FD004_path, test_FD004_path, RUL_FD004_path]\n","\n","## Assign columns name\n","cols = ['unit_nr', 'cycles', 'os_1', 'os_2', 'os_3']\n","cols += ['sensor_{0:02d}'.format(s + 1) for s in range(26)]\n","col_rul = ['RUL_truth']\n","\n","## Read csv file to pandas dataframe\n","FD_path = [\"none\", FD001_path, FD002_path, FD003_path, FD004_path]\n","FD_name = [\"none\", \"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n","\n","# Training settings\n","sequence_length = 30\n","batch_size = 512\n","epochs = 10\n","lr = [0.001, 0.01,0.05]\n","momentum = 0.9\n","cuda = True\n","seed = 8\n","log_interval = 5\n","l2_decay = 5e-4\n","\n","sensor_drop = ['sensor_01', 'sensor_05', 'sensor_06', 'sensor_10', 'sensor_16', 'sensor_18', 'sensor_19']\n","\n","source_path = [\"none\", FD_path[1], FD_path[2], FD_path[3], FD_path[4]]  #此处选择source 数据集 1 2 3 分别对应FD001-FD004\n","target_path = [\"none\", FD_path[1], FD_path[2], FD_path[3], FD_path[4]]\n","datasetset_name = [\"none\", \"FD001\", \"FD002\", \"FD003\", \"FD004\"]\n","\n","torch.manual_seed(seed)\n","if cuda:\n","    torch.cuda.manual_seed(seed)\n","\n"],"metadata":{"id":"0X9r6FSotPvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["source_chosen = ['None',1,3,4]\n","target_chosen = ['None',2]\n","\n","\n","source1_name = datasetset_name[source_chosen[1]]\n","source2_name = datasetset_name[source_chosen[2]]\n","source3_name = datasetset_name[source_chosen[3]]\n","\n","target_train_loader = data_loader.load_training(target_path[target_chosen[1]], sequence_length, sensor_drop,\n","                                                batch_size)\n"],"metadata":{"id":"34ES8_LEt_O0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","for inputs,label in target_train_loader:\n","    print(inputs,label)"],"metadata":{"id":"xxjWJGH_uBIf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["piecewise_lin_ref = 125\n","data_path_list = FD002_path\n","\n","\n","train_FD = pd.read_csv(data_path_list[0], sep=' ', header=None,\n","                        names=cols, index_col=False)\n","test_FD = pd.read_csv(data_path_list[1], sep=' ', header=None,\n","                      names=cols, index_col=False)\n","RUL_FD = pd.read_csv(data_path_list[2], sep=' ', header=None,\n","                      names=col_rul, index_col=False)\n","mapper = {}\n","for unit_nr in train_FD['unit_nr'].unique():\n","    mapper[unit_nr] = train_FD['cycles'].loc[train_FD['unit_nr'] == unit_nr].max()\n","\n","# calculate RUL = time.max() - time_now for each unit\n","train_FD['RUL'] = train_FD['unit_nr'].apply(lambda nr: mapper[nr]) - train_FD['cycles']\n","# print(train_FD['RUL'].describe())\n","# train_FD['RUL'].head(200)\n","\n","\n","# piecewise linear for RUL labels\n","train_FD['RUL'].loc[(train_FD['RUL'] > piecewise_lin_ref)] = piecewise_lin_ref\n","# print(train_FD['RUL'].describe())\n","\n","mapper2 = {}\n","for unit_nr in train_FD['unit_nr'].unique():\n","    mapper2[unit_nr] = train_FD['RUL'].loc[train_FD['unit_nr'] == unit_nr].max()\n","# calculate RUL = time.max() - time_now for each unit\n","train_FD['RUL'] = (train_FD['RUL'])/train_FD['unit_nr'].apply(lambda nr: mapper2[nr])\n","# piecewise linear for RUL labels\n","# train_FD['RUL'].describe()\n","#print(train_FD['RUL'])"],"metadata":{"id":"C1fDzX7teAbQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cut max RUL ground truth\n","RUL_FD['RUL_truth'].loc[(RUL_FD['RUL_truth'] > piecewise_lin_ref)] = piecewise_lin_ref\n","RUL_FD['RUL_truth']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8u0fiVnqeOSM","executionInfo":{"status":"ok","timestamp":1673103776446,"user_tz":-480,"elapsed":571,"user":{"displayName":"zm f","userId":"13806944385549967444"}},"outputId":"cbf890e8-5cef-443e-9b3b-deb241f1d70e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       18\n","1       79\n","2      106\n","3      110\n","4       15\n","      ... \n","254    122\n","255    125\n","256     56\n","257    125\n","258     51\n","Name: RUL_truth, Length: 259, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["## Excluse columns which only have NaN as value\n","# nan_cols = ['sensor_{0:02d}'.format(s + 22) for s in range(5)]\n","cols_nan = train_FD.columns[train_FD.isna().any()].tolist()\n","# print('Columns with all nan: \\n' + str(cols_nan) + '\\n')\n","cols_const = [col for col in train_FD.columns if len(train_FD[col].unique()) <= 2]  ###os3列在这个时候已经被去掉了\n","# print('Columns with all const values*******: \\n' + str(cols_const) + '\\n')\n","\n","## Drop exclusive columns\n","train_FD = train_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n","test_FD = test_FD.drop(columns=cols_const + cols_nan + sensor_drop)\n","\n","# 对数据进行Min—Max变换\n","\n","## preprocessing(normailization for the neural networks)\n","min_max_scaler = preprocessing.MinMaxScaler()\n","# for the training set\n","# train_FD['cycles_norm'] = train_FD['cycles']\n","cols_normalize = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL']) #求差集保留train_FD.columns\n","\n","norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_FD[cols_normalize]),\n","                              columns=cols_normalize,\n","                              index=train_FD.index)\n","join_df = train_FD[train_FD.columns.difference(cols_normalize)].join(norm_train_df)\n","train_FD = join_df.reindex(columns=train_FD.columns)\n","\n","# for the test set\n","# test_FD['cycles_norm'] = test_FD['cycles']\n","cols_normalize_test = test_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2'])\n","# print (\"cols_normalize_test\", cols_normalize_test)\n","norm_test_df = pd.DataFrame(min_max_scaler.transform(test_FD[cols_normalize_test]),\n","                            columns=cols_normalize_test,\n","                            index=test_FD.index)\n","test_join_df = test_FD[test_FD.columns.difference(cols_normalize_test)].join(norm_test_df)\n","test_FD = test_join_df.reindex(columns=test_FD.columns)\n","test_FD = test_FD.reset_index(drop=True)\n","\n","\n","\n","# Specify the columns to be used\n","sequence_cols_train = train_FD.columns.difference(['unit_nr', 'cycles', 'os_1', 'os_2', 'RUL'])\n","sequence_cols_test = test_FD.columns.difference(['unit_nr', 'os_1', 'os_2', 'cycles'])\n","# print(train_FD.columns.tolist(),test_FD.columns.tolist(),sequence_cols_train,sequence_cols_test)\n","## generator for the sequences\n","# transform each id of the train dataset in a sequence\n","seq_gen = (list(gen_sequence(train_FD[train_FD['unit_nr'] == id], sequence_length, sequence_cols_train))\n","            for id in train_FD['unit_nr'].unique())\n","\n","# generate sequences and convert to numpy array in training set\n","seq_array_train = np.concatenate(list(seq_gen)).astype(np.float32)\n","seq_array_train = seq_array_train.transpose(0, 2, 1)  # shape = (samples, sensors, sequences)\n","# print(\"seq_array_train.shape\", self.seq_array_train.shape)\n","# generate label of training samples\n","label_gen = [gen_labels(train_FD[train_FD['unit_nr'] == id], sequence_length, ['RUL'])\n","              for id in train_FD['unit_nr'].unique()]\n","label_array_train = np.concatenate(label_gen).astype(np.float32)\n"],"metadata":{"id":"gm27gha6iwo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate sequences and convert to numpy array in test set (only the last sequence for each engine in test set)\n","seq_array_test_last = [test_FD[test_FD['unit_nr'] == id][sequence_cols_test].values[-sequence_length:]\n","                        for id in test_FD['unit_nr'].unique() if\n","                        len(test_FD[test_FD['unit_nr'] == id]) >= sequence_length]\n","seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n","seq_array_test_last = seq_array_test_last.transpose(0, 2, 1)  "],"metadata":{"id":"1sxypj9Djo-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_mask = [len(test_FD[test_FD['unit_nr'] == id]) >= sequence_length for id in test_FD['unit_nr'].unique()]\n","print(len(y_mask))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TaOdxQwOj8AA","executionInfo":{"status":"ok","timestamp":1673103805883,"user_tz":-480,"elapsed":691,"user":{"displayName":"zm f","userId":"13806944385549967444"}},"outputId":"db4e888c-b600-4cc7-aa77-8e7b810de456"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["259\n"]}]},{"cell_type":"code","source":["mapper1 = {}\n","for unit_nr in test_FD['unit_nr'].unique():\n","    mapper1[unit_nr] = test_FD['cycles'].loc[test_FD['unit_nr'] == unit_nr].max()\n","mapper1_df = pd.DataFrame(list(mapper1.items()))\n"],"metadata":{"id":"FpgZhceVkGdg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testmax = test_FD['unit_nr'].drop_duplicates(keep='first')\n","\n","# testmax['2'] = mapper1_df[1]\n","# testmax = mapper1_df[1]\n","testmax = testmax.reset_index()\n","testmax.loc[:, 'maxlen'] = 0\n","testmax['maxlen'] = mapper1_df[1]\n","# for i in range(len(mapper1_df[1])):\n","#   testmax['maxlen'][i] = mapper1_df[1][i]\n","# pd.merge(testmax, mapper1_df,left_index=True, right_index=True, how='outer')\n","# pd.concat([testmax, mapper1_df], axis=1，join=outer )\n","testmax"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"XE80wNz_t3TI","executionInfo":{"status":"ok","timestamp":1673103813426,"user_tz":-480,"elapsed":570,"user":{"displayName":"zm f","userId":"13806944385549967444"}},"outputId":"6fde6c42-0057-41c1-cd2e-5afc2ecc8a8f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     index  unit_nr  maxlen\n","0        0        1     258\n","1      258        2      55\n","2      313        3     165\n","3      478        4      86\n","4      564        5     148\n","..     ...      ...     ...\n","254  33362      255     150\n","255  33512      256      59\n","256  33571      257     199\n","257  33770      258      98\n","258  33868      259     123\n","\n","[259 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-60daf025-c9ab-4753-b996-48af4a61cd9f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>unit_nr</th>\n","      <th>maxlen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>258</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>258</td>\n","      <td>2</td>\n","      <td>55</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>313</td>\n","      <td>3</td>\n","      <td>165</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>478</td>\n","      <td>4</td>\n","      <td>86</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>564</td>\n","      <td>5</td>\n","      <td>148</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>254</th>\n","      <td>33362</td>\n","      <td>255</td>\n","      <td>150</td>\n","    </tr>\n","    <tr>\n","      <th>255</th>\n","      <td>33512</td>\n","      <td>256</td>\n","      <td>59</td>\n","    </tr>\n","    <tr>\n","      <th>256</th>\n","      <td>33571</td>\n","      <td>257</td>\n","      <td>199</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>33770</td>\n","      <td>258</td>\n","      <td>98</td>\n","    </tr>\n","    <tr>\n","      <th>258</th>\n","      <td>33868</td>\n","      <td>259</td>\n","      <td>123</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>259 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60daf025-c9ab-4753-b996-48af4a61cd9f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-60daf025-c9ab-4753-b996-48af4a61cd9f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-60daf025-c9ab-4753-b996-48af4a61cd9f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["label_array_test_last = RUL_FD['RUL_truth'][y_mask].values/(RUL_FD['RUL_truth'][y_mask].values+testmax['maxlen'][y_mask].values)\n","label_array_test = label_array_test_last.reshape(label_array_test_last.shape[0], 1).astype(np.float32)\n","#print(\"label_array_test.shape\", self.label_array_test.shape)\n","#print(self.label_array_test.shape)"],"metadata":{"id":"-nu3ofjjkpv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ywCelwiRxOKB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model):\n","    global mmd_loss1,mmd_loss2,mmd_loss3\n","    print(\"--------------------------MFSAN --------------------------------\")\n","\n","    f_mfsan_train = log_in_file(\"/mfsan_train_log.log\")\n","\n","    source1_loader = data_loader.load_training(source_path[source_chosen[1]], sequence_length, sensor_drop,\n","                                                batch_size)\n","    source2_loader = data_loader.load_training(source_path[source_chosen[2]], sequence_length, sensor_drop,\n","                                                batch_size)\n","    source3_loader = data_loader.load_training(source_path[source_chosen[3]], sequence_length, sensor_drop,\n","                                               batch_size)\n","\n","    source1_len = len(source1_loader)\n","    source2_len = len(source2_loader)\n","    source3_len = len(source3_loader)\n","\n","    target_len = len(target_train_loader)\n","    max_len = max(source1_len,source2_len,source3_len,target_len)\n","    # print(\"111111\",source1_len,source2_len,target_len,max_len)\n","\n","    batch_src1,batch_src2,batch_src3,batch_tar = 0,0,0,0\n","    running_loss_scr1, running_loss_scr2 ,running_loss_scr3= 0, 0, 0\n","    list_src1, list_src2, list_src3, list_tar = list(enumerate(source1_loader)), list(enumerate(source2_loader)), list(enumerate(source3_loader)),list(enumerate(target_train_loader))\n","\n","    optimizer = torch.optim.SGD([\n","        {'params': model.sharedNet.parameters(), 'lr': lr[1]},\n","\n","        {'params': model.rul_fc_son1.parameters(), 'lr': lr[1]},\n","        {'params': model.rul_fc_son2.parameters(), 'lr': lr[2]},\n","\n","        {'params': model.sonnet1.parameters(), 'lr': lr[1]},\n","        {'params': model.sonnet2.parameters(), 'lr': lr[2]},\n","\n","    ], lr=lr[2], momentum=momentum, weight_decay=l2_decay)\n","\n","\n","\n","    for batch in range(1, max_len):\n","        model.train()\n","\n","        p = (batch - 1) / (max_len)\n","        alpha = 2. / (1. + np.exp(-10 * p)) - 1\n","        optimizer = utils.optimizer_scheduler(optimizer=optimizer, p=p)\n","        optimizer.zero_grad()\n","\n","        #####scr1 tgt\n","        _, (source_data1, source_label1) = list_src1[batch_src1]\n","        _, (target_data1, _) = list_tar[batch_tar]\n","\n","\n","        if cuda:\n","            source_data1, source_label1 = source_data1.cuda(), source_label1.type(torch.FloatTensor).cuda()\n","            target_data1 = target_data1.cuda()\n","        source_data1, source_label1 = Variable(source_data1), Variable(source_label1) #tensor不能反向传播，variable可以反向传播\n","        target_data1 = Variable(target_data1)\n","\n","        rul_loss1, mmd_loss1, l1_loss1 = model(source_data1, target_data1, source_label1, alpha, mark=1)\n","        gamma = 2 / (1 + math.exp(-10 * (batch) / (max_len))) - 1\n","        loss1 = rul_loss1 + gamma * (mmd_loss1 + l1_loss1)\n","        loss1.backward()\n","        optimizer.step()\n","        running_loss_scr1 += loss1.item()\n","\n","        batch_src1 += 1\n","        if batch_src1 >= len(list_src1)-1:\n","            batch_src1 = 0\n","\n","        if batch_tar >= len(list_tar)-1:\n","            batch_tar = 0\n","\n","        if batch % log_interval == 0:\n","            print('Train source1 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\tRUL_Loss: {:.6f}\\tmmd_Loss: {:.6f}\\tl1_Loss: {:.6f}'.format(\n","                batch, 100. * batch/ max_len, loss1.item(), rul_loss1.item(), mmd_loss1.item(), l1_loss1.item()), file=f_mfsan_train, flush=True)\n","\n","        # print(\"batch_src1,batch_tar\",batch_src1,batch_tar)\n","\n","        #####scr2 tgt\n","        _, (source_data2, source_label2) = list_src2[batch_src2]\n","        _, (target_data2, _) = list_tar[batch_tar]\n","\n","        if cuda:\n","            source_data2, source_label2 = source_data2.cuda(), source_label2.type(torch.FloatTensor).cuda()\n","            target_data2 = target_data2.cuda()\n","        source_data2, source_label2 = Variable(source_data2), Variable(source_label2)\n","        target_data2 = Variable(target_data2)\n","        optimizer.zero_grad()\n","\n","        rul_loss2, mmd_loss2, l1_loss2 = model(source_data2, target_data2, source_label2, alpha, mark=2)\n","        gamma = 2 / (1 + math.exp(-10 * (batch) / (max_len))) - 1\n","        loss2 = rul_loss2 + gamma * (mmd_loss2 + l1_loss2)\n","        loss2.backward()\n","        optimizer.step()\n","        running_loss_scr2 += loss2.item()\n","\n","        batch_src2 += 1\n","        if batch_src2 >= len(list_src2)-1:\n","            batch_src2 = 0\n","\n","\n","        if batch_tar >= len(list_tar)-1:\n","            batch_tar = 0\n","        # print(\"batch_src2,batch_tar\", batch_src2, batch_tar)\n","\n","        if batch % log_interval == 0:\n","            print(\n","                'Train source2 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\tRUL_Loss: {:.6f}\\tmmd_Loss: {:.6f}\\tl1_Loss: {:.6f}'.format(\n","                    batch, 100. * batch / max_len, loss2.item(), rul_loss2.item(), mmd_loss2.item(), l1_loss2.item()), file=f_mfsan_train, flush=True)\n","\n","        #####scr3 tgt\n","        _, (source_data3, source_label3) = list_src3[batch_src3]\n","        _, (target_data3, _) = list_tar[batch_tar]\n","\n","        if cuda:\n","            source_data3, source_label3 = source_data3.cuda(), source_label3.type(torch.FloatTensor).cuda()\n","            target_data3 = target_data3.cuda()\n","        source_data3, source_label3 = Variable(source_data3), Variable(source_label3)\n","        target_data3 = Variable(target_data3)\n","        optimizer.zero_grad()\n","\n","        rul_loss3, mmd_loss3, l1_loss3 = model(source_data3, target_data3, source_label3, alpha, mark=3)\n","        gamma = 2 / (1 + math.exp(-10 * (batch) / (max_len))) - 1\n","        loss3 = rul_loss3 + gamma * (mmd_loss3 + l1_loss3)\n","        loss3.backward()\n","        optimizer.step()\n","        running_loss_scr3 += loss3.item()\n","\n","        batch_src3 += 1\n","        if batch_src3 >= len(list_src3) - 1:\n","            batch_src3 = 0\n","\n","        batch_tar += 1\n","        if batch_tar >= len(list_tar) - 1:\n","            batch_tar = 0\n","        # print(\"batch_src2,batch_tar\", batch_src2, batch_tar)\n","\n","        if batch % log_interval == 0:\n","            print(\n","                'Train source3 iter: {} [({:.0f}%)]\\tLoss: {:.6f}\\tRUL_Loss: {:.6f}\\tmmd_Loss: {:.6f}\\tl1_Loss: {:.6f}\\n\\n'.format(\n","                    batch, 100. * batch / max_len, loss3.item(), rul_loss3.item(), mmd_loss3.item(), l1_loss3.item()),\n","                file=f_mfsan_train, flush=True)\n","\n","    f_mfsan_train.close()\n","    epoch_loss_scr1 = running_loss_scr1 / (max_len - 1)\n","    epoch_loss_scr2 = running_loss_scr2 / (max_len - 1)\n","    epoch_loss_scr3 = running_loss_scr3 / (max_len - 1)\n","\n","    return epoch_loss_scr1, epoch_loss_scr2, epoch_loss_scr3\n","\n","\n","def test(model):\n","    model.eval()\n","\n","    f_mfsan_test = log_in_file(\"/mfsan_test_log.log\")\n","\n","    with torch.no_grad():\n","        for data, target in target_test_loader:\n","            if cuda:\n","                data, target = data.cuda(), target.type(torch.FloatTensor).cuda()\n","            data, target = Variable(data), Variable(target)\n","            pred1, pred2, pred3 = model(data)\n","\n","            pred1, pred2, pred3 =pred1.detach().cpu().numpy().squeeze(1), pred2.detach().cpu().numpy().squeeze(1), pred3.detach().cpu().numpy().squeeze(1)\n","            target = target.detach().cpu().numpy()\n","\n","            rmse1 = rmse_cal(pred1, target)  # sum up batch loss\n","            score1 = score_cal(pred1, target)\n","            rmse2 = rmse_cal(pred2, target) # sum up batch loss\n","            score2 = score_cal(pred2, target)\n","            rmse3 = rmse_cal(pred3, target)  # sum up batch loss\n","            score3 = score_cal(pred3, target)\n","            #w1 = (mmd_loss1-0.5) ** (-2)\n","            #w2 = (mmd_loss2-0.5) ** (-2)\n","            #w3 = (mmd_loss3-0.5) ** (-2)\n","            w1 = mmd_loss1 ** (-1)\n","            w2 = mmd_loss2 ** (-1)\n","            w3 = mmd_loss3 ** (-1)\n","            #w1 = mmd_loss1\n","            #w2 = mmd_loss2\n","            #w3 = mmd_loss3\n","            w1 = w1.cpu().numpy()\n","            w2 = w2.cpu().numpy()\n","            w3 = w3.cpu().numpy()\n","            ws=w1+w2+w3\n","\n","            #pred = (w1*pred1 + w2*pred2 + w3*pred3) / ws\n","            pred=(pred1+pred2+pred3)/3\n","            rmse = rmse_cal(pred, target) # sum up batch loss\n","            score = score_cal(pred, target)\n","\n","\n","        print(target_name, '\\nTest set: Mutil_source rmse: {:.4f}, Mutil_source score: {:.4f}'.format(rmse, score), file=f_mfsan_test, flush=True)\n","        print('source1 rmse {:.4f}, source2 rmse {:.4f}, source3 rmse {:.4f}'.format(rmse1, rmse2,rmse3), file=f_mfsan_test, flush=True)\n","        print('source1 score {:.4f}, source2 score {:.4f}, source3 score {:.4f}\\n\\n'.format(score1, score2,score3), file=f_mfsan_test, flush=True)\n","    return rmse, score,rmse1,score1,rmse2,score2,rmse3,score3,pred1, pred2, pred3, pred,target\n"],"metadata":{"id":"jicgnRWxTpf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = model_mfsan.MFSAN().apply(weight_init)"],"metadata":{"id":"u9WJx72e09Oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ci-bCd3TttL","executionInfo":{"status":"ok","timestamp":1673103855689,"user_tz":-480,"elapsed":2,"user":{"displayName":"zm f","userId":"13806944385549967444"}},"outputId":"e56eb543-9d00-43fc-a494-73459c1c1c8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MFSAN(\n","  (sharedNet): CFE(\n","    (conv1): Conv1d(14, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n","    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)\n","    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv3): Conv1d(64, 128, kernel_size=(2,), stride=(1,), bias=False)\n","    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","  )\n","  (sonnet1): DFE(\n","    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n","  )\n","  (sonnet2): DFE(\n","    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n","  )\n","  (sonnet3): DFE(\n","    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n","  )\n","  (rul_fc_son1): Linear(in_features=199, out_features=1, bias=True)\n","  (rul_fc_son2): Linear(in_features=199, out_features=1, bias=True)\n","  (rul_fc_son3): Linear(in_features=199, out_features=1, bias=True)\n","  (avgpool): AvgPool1d(kernel_size=(2,), stride=(1,), padding=(0,))\n","  (dann): Discriminator(\n","    (discriminator): Sequential(\n","      (0): Linear(in_features=199, out_features=50, bias=True)\n","      (1): ReLU()\n","      (2): Dropout(p=0.2, inplace=False)\n","      (3): Linear(in_features=50, out_features=10, bias=True)\n","      (4): ReLU()\n","      (5): Dropout(p=0.2, inplace=False)\n","      (6): Linear(in_features=10, out_features=2, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[],"metadata":{"id":"HPKSepaBwstA"},"execution_count":null,"outputs":[]}]}