当前日期和时间： 2023-03-15_12_05_52
training settings:	 lr: 0.001 momentum: 0.9 l2_decay: 0.001 optimizer: Adam seed: 6
model architecture:
 MFSAN(
  (sharedNet): CFE(
    (conv1): Conv1d(14, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
    (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv2): Conv1d(32, 64, kernel_size=(2,), stride=(1,), padding=(1,), bias=False)
    (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (conv3): Conv1d(64, 128, kernel_size=(2,), stride=(1,), bias=False)
    (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (sonnet1): DFE(
    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)
  )
  (sonnet2): DFE(
    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)
  )
  (sonnet3): DFE(
    (rnn): LSTM(128, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)
  )
  (rul_fc_son1): Linear(in_features=199, out_features=1, bias=True)
  (rul_fc_son2): Linear(in_features=199, out_features=1, bias=True)
  (rul_fc_son3): Linear(in_features=199, out_features=1, bias=True)
  (avgpool): AvgPool1d(kernel_size=(2,), stride=(1,), padding=(0,))
)
FD001 FD002 FD004 to FD003
Train source1 iter: 10 [(9%)]	Loss: 0.067647	RUL_Loss: 0.036031	mmd_Loss: 0.032684	l1_Loss: 0.039239
Train source2 iter: 10 [(9%)]	Loss: 0.145108	RUL_Loss: 0.121553	mmd_Loss: 0.025177	l1_Loss: 0.028409
Train source3 iter: 10 [(9%)]	Loss: 0.376080	RUL_Loss: 0.324660	mmd_Loss: 0.062124	l1_Loss: 0.054853


Train source1 iter: 20 [(19%)]	Loss: 0.084680	RUL_Loss: 0.035117	mmd_Loss: 0.031190	l1_Loss: 0.036078
Train source2 iter: 20 [(19%)]	Loss: 0.149509	RUL_Loss: 0.111967	mmd_Loss: 0.023456	l1_Loss: 0.027497
Train source3 iter: 20 [(19%)]	Loss: 0.390081	RUL_Loss: 0.308333	mmd_Loss: 0.058046	l1_Loss: 0.052907


Train source1 iter: 30 [(28%)]	Loss: 0.093969	RUL_Loss: 0.032164	mmd_Loss: 0.034415	l1_Loss: 0.035141
Train source2 iter: 30 [(28%)]	Loss: 0.166546	RUL_Loss: 0.119120	mmd_Loss: 0.026647	l1_Loss: 0.026726
Train source3 iter: 30 [(28%)]	Loss: 0.414508	RUL_Loss: 0.309017	mmd_Loss: 0.067073	l1_Loss: 0.051647


Train source1 iter: 40 [(38%)]	Loss: 0.097193	RUL_Loss: 0.033325	mmd_Loss: 0.032454	l1_Loss: 0.034417
Train source2 iter: 40 [(38%)]	Loss: 0.169838	RUL_Loss: 0.122066	mmd_Loss: 0.022971	l1_Loss: 0.027048
Train source3 iter: 40 [(38%)]	Loss: 0.412525	RUL_Loss: 0.306476	mmd_Loss: 0.059096	l1_Loss: 0.051940


